{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "experimental-monroe",
   "metadata": {},
   "source": [
    "## name-gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "numerous-heath",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from sklearn.metrics import classification_report, f1_score,accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping\n",
    "from nltk import word_tokenize\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import time\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "multiple-xerox",
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.read_csv('name_gender.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-wichita",
   "metadata": {},
   "source": [
    "## simple EDA and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "sealed-provider",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95025, 2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "magnetic-skirt",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "F    60304\n",
       "M    34721\n",
       "Name: gender, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the dataset is imblanced, can try using different imbalanced handling method later\n",
    "df['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "juvenile-perry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aaban&amp;&amp;</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aabha*</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aabid</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aabriella</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aada_</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        name gender\n",
       "0    Aaban&&      M\n",
       "1     Aabha*      F\n",
       "2      Aabid      M\n",
       "3  Aabriella      F\n",
       "4      Aada_      F"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a simple observation: the first 3 female name all end with 'a', \n",
    "# suggesting that the last letter might be important feature\n",
    "df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "consistent-orleans",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text cleansing\n",
    "def preprocessing(s):\n",
    "    # 1.to lower\n",
    "    s=s.lower()\n",
    "    # 2: keep only the letter \n",
    "    s =''.join([w for w in s if w.isalpha()])\n",
    "    return s\n",
    "\n",
    "df['clean_name']=df['name'].apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "vocational-trash",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95025"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#no duplicates in the name\n",
    "df['clean_name'].nunique() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "amateur-wedding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    95025.000000\n",
       "mean         6.534070\n",
       "std          1.486065\n",
       "min          2.000000\n",
       "25%          6.000000\n",
       "50%          6.000000\n",
       "75%          7.000000\n",
       "max         15.000000\n",
       "Name: name_len, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['name_len']=df['clean_name'].apply(lambda x: len(x))\n",
    "df['name_len'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-terminal",
   "metadata": {},
   "source": [
    "(1)Based on common sense and research literature, there is no correlation between the gender and percentage of letter in a name. Therefore the is no point to include n-gram = 1. \n",
    "(2)the median length of name in the dataset is 6. Therefore when using CountVectorizer, I will not set a ngram range larger than 6, (2,6) will be the largest range. \n",
    "(3)Since every name is considerably short, certain pattern is likely to appear only once. There is no point in setting min-df larger than 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artistic-exhaust",
   "metadata": {},
   "source": [
    "## baseline Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "accessory-luxembourg",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['clean_name']\n",
    "y = df['gender']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=1004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "allied-treat",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_cv = CountVectorizer(analyzer='char',ngram_range=(2,6))\n",
    "X_train_cv=vect_cv.fit_transform(X_train)\n",
    "X_val_cv=vect_cv.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "theoretical-seeker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           F       0.84      0.88      0.86     12055\n",
      "           M       0.77      0.70      0.73      6950\n",
      "\n",
      "    accuracy                           0.81     19005\n",
      "   macro avg       0.80      0.79      0.79     19005\n",
      "weighted avg       0.81      0.81      0.81     19005\n",
      "\n",
      "0.8131018153117601\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.793704105881583e-08"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb=MultinomialNB()\n",
    "nb.fit(X_train_cv,y_train)\n",
    "y_pred = nb.predict(X_val_cv)\n",
    "print(classification_report(y_true=y_val,y_pred=y_pred))\n",
    "print(accuracy_score(y_val,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-wrong",
   "metadata": {},
   "source": [
    "The accuracy for baseline model is  0.813. Note that the f1-score for female is significatly higher than male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "republican-publicity",
   "metadata": {},
   "outputs": [],
   "source": [
    "## see if the score can improve by handling imbalance problem\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "over = RandomOverSampler(random_state=1004)\n",
    "under = RandomUnderSampler(random_state=1004)\n",
    "X_train_cv_over, y_train_over=over.fit_resample(X_train_cv,y_train)\n",
    "X_train_cv_under, y_train_under=under.fit_resample(X_train_cv,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "unusual-province",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           F       0.86      0.84      0.85     12055\n",
      "           M       0.74      0.77      0.75      6950\n",
      "\n",
      "    accuracy                           0.81     19005\n",
      "   macro avg       0.80      0.80      0.80     19005\n",
      "weighted avg       0.82      0.81      0.81     19005\n",
      "\n",
      "0.8138910812943962\n"
     ]
    }
   ],
   "source": [
    "nb.fit(X_train_cv_over, y_train_over)\n",
    "y_pred = nb.predict(X_val_cv)\n",
    "print(classification_report(y_true=y_val,y_pred=y_pred))\n",
    "print(accuracy_score(y_val,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developmental-editor",
   "metadata": {},
   "source": [
    "baseline model with random oversampling: slight improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dated-hormone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           F       0.86      0.83      0.84     12055\n",
      "           M       0.72      0.78      0.75      6950\n",
      "\n",
      "    accuracy                           0.81     19005\n",
      "   macro avg       0.79      0.80      0.80     19005\n",
      "weighted avg       0.81      0.81      0.81     19005\n",
      "\n",
      "0.8073664825046041\n"
     ]
    }
   ],
   "source": [
    "nb.fit(X_train_cv_under, y_train_under)\n",
    "y_pred = nb.predict(X_val_cv)\n",
    "print(classification_report(y_true=y_val,y_pred=y_pred))\n",
    "print(accuracy_score(y_val,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "small-location",
   "metadata": {},
   "source": [
    "Baseline model with random undersampling:  drop "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desperate-determination",
   "metadata": {},
   "source": [
    "## trial hyper parameter tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tamil-letters",
   "metadata": {},
   "source": [
    "using imblearn instead of sklearn since the pipeline in sklearn cannot change the size of input data,\n",
    "i.e.: can not include the random oversampling step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ambient-titanium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 110 candidates, totalling 550 fits\n",
      "time takes in minutes: 11.7\n",
      "{'nb__alpha': 0.1, 'vect': CountVectorizer(analyzer='char', ngram_range=(2, 6)), 'vect__ngram_range': (2, 6)}\n",
      "0.8112996579847408\n"
     ]
    }
   ],
   "source": [
    "from imblearn.pipeline import make_pipeline, Pipeline\n",
    "start = time.time()\n",
    "pipe = Pipeline([('vect', CountVectorizer()), ('over', over), ('nb', MultinomialNB())])\n",
    "pipe_param= [\n",
    "   {'vect': [CountVectorizer(analyzer='char')],\n",
    "    'vect__ngram_range':   [(2,2),(2,3),(2,4),(2,5),(2,6)],\n",
    "    'nb__alpha':     np.linspace(0,1,11) },\n",
    "    \n",
    "   {'vect': [TfidfVectorizer(analyzer='char')],\n",
    "    'vect__ngram_range':   [(2,2),(2,3),(2,4),(2,5),(2,6)],\n",
    "    'nb__alpha':     np.linspace(0,1,11)}]\n",
    "\n",
    "pipe_grid =GridSearchCV(estimator=pipe,\n",
    "                        param_grid=pipe_param,\n",
    "                        cv=5,\n",
    "                        verbose=1,\n",
    "                        scoring='accuracy')\n",
    "\n",
    "pipe_grid.fit(X_train,y_train)\n",
    "\n",
    "print(\"time takes in minutes:\",round((time.time()-start)/60,1))\n",
    "print(pipe_grid.best_params_)\n",
    "print(pipe_grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "civilian-macedonia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           F       0.86      0.86      0.86     12055\n",
      "           M       0.75      0.75      0.75      6950\n",
      "\n",
      "    accuracy                           0.82     19005\n",
      "   macro avg       0.81      0.81      0.81     19005\n",
      "weighted avg       0.82      0.82      0.82     19005\n",
      "\n",
      "0.8192054722441463\n"
     ]
    }
   ],
   "source": [
    "# selected nb\n",
    "nb=MultinomialNB(alpha=0.1)\n",
    "nb.fit(X_train_cv_over, y_train_over)\n",
    "y_pred = nb.predict(X_val_cv)\n",
    "print(classification_report(y_true=y_val,y_pred=y_pred))\n",
    "print(accuracy_score(y_val,y_pred)) #slightly better than before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apart-granny",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flush-facial",
   "metadata": {},
   "source": [
    "According to literature review, The most signifiant pattern to differentiate the gender based on name is the last few letters. Besides this, 'ild' and 'lin' are common pattern that in girls name metioned in literature which might not necessarily be at the end part of the name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "anonymous-saskatchewan",
   "metadata": {},
   "outputs": [],
   "source": [
    "## add some pattern according to research paper\n",
    "df['last_letter']=df['clean_name'].apply(lambda x: x[-1])\n",
    "df['last_2_letter']=df['clean_name'].apply(lambda x:x[-2:])\n",
    "df['last_3_letter']=df['clean_name'].apply(lambda x:x[-3:])\n",
    "df['ild']=df['clean_name'].apply(lambda x: bool(re.search('lid',x)))\n",
    "df['lin']=df['clean_name'].apply(lambda x: bool(re.search('lin',x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "covered-sampling",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df.copy() #save of copy of dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "recreational-madness",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4366"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#one hot encoding for ['last_letter','last_2_letter','last_3_letter']\n",
    "#features_col=['last_letter','last_2_letter','last_3_letter']\n",
    "#df=pd.get_dummies(data=df, columns=features_col, dtype=bool, drop_first=False)\n",
    "#len(list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "painted-battery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4366"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "features_col=['last_letter','last_2_letter','last_3_letter']\n",
    "onehot=OneHotEncoder(handle_unknown='ignore') \n",
    "new_cols = onehot.fit_transform(df[features_col]).toarray()\n",
    "names_col=onehot.get_feature_names(features_col)\n",
    "df_new= pd.DataFrame(new_cols,dtype=bool, columns=names_col)\n",
    "df=pd.concat([df,df_new],axis=1).drop(columns=features_col)\n",
    "len(list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "environmental-narrative",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['gender','name','name_len'])\n",
    "y = df['gender']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=1004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "atomic-theory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to concat cv/tfv character matrix and the manuel crafted features\n",
    "import scipy as sp\n",
    "def combine_features(X,X_cv):\n",
    "    X_manual = X.drop(columns=['clean_name'])\n",
    "    X_manual=  X_manual.fillna(0)\n",
    "    X_manual_sparse = sp.sparse.csr_matrix(X_manual)\n",
    "    X_full = sp.sparse.hstack([X_cv, X_manual_sparse])\n",
    "    return X_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "improving-immune",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to transfrom train and val text to cv/tdidf matrix\n",
    "def vect_transfrom(vect, X_train, X_val):\n",
    "    X_train_m = vect.fit_transform(X_train['clean_name'])\n",
    "    X_val_m = vect.transform(X_val['clean_name'])\n",
    "    return X_train_m, X_val_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "vocational-addiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to print result and check overfitting\n",
    "def model_result(model, X_train, y_train, X_val, y_val):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_val=model.predict(X_val)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    print(classification_report(y_true=y_val,y_pred=y_pred_val))\n",
    "    print(\"testing score is\", accuracy_score(y_val,y_pred_val))\n",
    "    print(\"training score is\",accuracy_score(y_train,y_pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nuclear-circumstances",
   "metadata": {},
   "source": [
    "### base model results by TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "amber-gender",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_tfv=TfidfVectorizer(analyzer='char',ngram_range=(2,6))\n",
    "X_train_tfv, X_val_tfv = vect_transfrom(vect_tfv, X_train, X_val)\n",
    "X_train_full = combine_features(X_train, X_train_tfv)\n",
    "X_val_full = combine_features(X_val, X_val_tfv)\n",
    "X_train_full_over, y_train_over = over.fit_resample(X_train_full, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "automotive-apartment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           F       0.92      0.89      0.91     12055\n",
      "           M       0.82      0.87      0.85      6950\n",
      "\n",
      "    accuracy                           0.88     19005\n",
      "   macro avg       0.87      0.88      0.88     19005\n",
      "weighted avg       0.89      0.88      0.88     19005\n",
      "\n",
      "testing score is 0.8837674296237832\n",
      "training score is 0.9241953201102614\n"
     ]
    }
   ],
   "source": [
    "# the best nb model by grid search\n",
    "nb=MultinomialNB(0.1)\n",
    "model_result(nb,X_train_full_over, y_train_over, X_val_full,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "sufficient-contents",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           F       0.93      0.90      0.91     12055\n",
      "           M       0.83      0.89      0.86      6950\n",
      "\n",
      "    accuracy                           0.89     19005\n",
      "   macro avg       0.88      0.89      0.89     19005\n",
      "weighted avg       0.90      0.89      0.89     19005\n",
      "\n",
      "testing score is 0.8939226519337017\n",
      "training score is 0.9305374204646728\n"
     ]
    }
   ],
   "source": [
    "## logistic regression\n",
    "lr = LogisticRegression(random_state=1004)\n",
    "model_result(lr,X_train_full_over, y_train_over, X_val_full,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "aboriginal-employee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           F       0.93      0.92      0.92     12055\n",
      "           M       0.86      0.88      0.87      6950\n",
      "\n",
      "    accuracy                           0.90     19005\n",
      "   macro avg       0.90      0.90      0.90     19005\n",
      "weighted avg       0.91      0.90      0.90     19005\n",
      "\n",
      "testing score is 0.904709287029729\n",
      "training score is 0.9919791083753031\n"
     ]
    }
   ],
   "source": [
    "## linear SVC\n",
    "## overfitting occured\n",
    "from sklearn.svm import LinearSVC\n",
    "svc = LinearSVC()\n",
    "model_result(svc,X_train_full_over, y_train_over, X_val_full,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "treated-tokyo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           F       0.93      0.86      0.90     12055\n",
      "           M       0.79      0.89      0.84      6950\n",
      "\n",
      "    accuracy                           0.87     19005\n",
      "   macro avg       0.86      0.88      0.87     19005\n",
      "weighted avg       0.88      0.87      0.87     19005\n",
      "\n",
      "testing score is 0.8720862930807682\n",
      "training score is 0.9014072830524985\n"
     ]
    }
   ],
   "source": [
    "## Xgboost\n",
    "xgb1 = xgb.XGBClassifier(random_state=1004)\n",
    "model_result(xgb1,X_train_full_over.tocsc(), y_train_over, X_val_full.tocsc(),y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accompanied-consciousness",
   "metadata": {},
   "source": [
    "### base model results by CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "center-oregon",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_cv=CountVectorizer(analyzer='char',ngram_range=(2,6))\n",
    "X_train_cv, X_val_cv = vect_transfrom(vect_cv, X_train, X_val)\n",
    "X_train_full = combine_features(X_train, X_train_cv)\n",
    "X_val_full = combine_features(X_val, X_val_cv)\n",
    "X_train_full_over, y_train_over = over.fit_resample(X_train_full, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "finnish-tokyo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           F       0.90      0.90      0.90     12055\n",
      "           M       0.83      0.83      0.83      6950\n",
      "\n",
      "    accuracy                           0.87     19005\n",
      "   macro avg       0.86      0.87      0.87     19005\n",
      "weighted avg       0.87      0.87      0.87     19005\n",
      "\n",
      "testing score is 0.8746645619573796\n",
      "training score is 0.9464341229870049\n"
     ]
    }
   ],
   "source": [
    "model_result(nb, X_train_full_over, y_train_over, X_val_full,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "trying-protocol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           F       0.94      0.91      0.92     12055\n",
      "           M       0.86      0.89      0.87      6950\n",
      "\n",
      "    accuracy                           0.91     19005\n",
      "   macro avg       0.90      0.90      0.90     19005\n",
      "weighted avg       0.91      0.91      0.91     19005\n",
      "\n",
      "testing score is 0.9057616416732439\n",
      "training score is 0.9801653920288503\n"
     ]
    }
   ],
   "source": [
    "model_result(lr,X_train_full_over, y_train_over, X_val_full,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "limiting-protein",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           F       0.92      0.92      0.92     12055\n",
      "           M       0.86      0.86      0.86      6950\n",
      "\n",
      "    accuracy                           0.90     19005\n",
      "   macro avg       0.89      0.89      0.89     19005\n",
      "weighted avg       0.90      0.90      0.90     19005\n",
      "\n",
      "testing score is 0.8963956853459616\n",
      "training score is 0.998424837820473\n"
     ]
    }
   ],
   "source": [
    "model_result(svc,X_train_full_over, y_train_over, X_val_full,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "spiritual-contamination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           F       0.93      0.87      0.90     12055\n",
      "           M       0.79      0.89      0.84      6950\n",
      "\n",
      "    accuracy                           0.88     19005\n",
      "   macro avg       0.86      0.88      0.87     19005\n",
      "weighted avg       0.88      0.88      0.88     19005\n",
      "\n",
      "testing score is 0.8751381215469614\n",
      "training score is 0.8930547783373749\n"
     ]
    }
   ],
   "source": [
    "model_result(xgb1,X_train_full_over.tocsc(), y_train_over, X_val_full.tocsc(),y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-radiation",
   "metadata": {},
   "source": [
    "1.the new featuers largely increased the score.  \n",
    "2.TFIDF leads to better test score and lower overfitting compared to CountVectorizer after feature engineering.  \n",
    "3.Among the models, LR(Countvectorizer) and linearSVC have serious overfitting problem. LR(TFIDF) has the best testing result, therefore LR will be tuned for improvement in later section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appointed-ladder",
   "metadata": {},
   "source": [
    "## tune logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "steady-metallic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time takes: 486.1\n",
      "{'C': 0.6000000000000001, 'max_iter': 80, 'penalty': 'none', 'solver': 'saga', 'warm_start': False}\n",
      "0.8996316758747698\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "lr = LogisticRegression()\n",
    "param_lr= {\n",
    "    'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "    'C': np.linspace(0.4, 1.1, 8),\n",
    "    'max_iter': [80, 100, 120, 150],\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "    'warm_start': [False, True]                                \n",
    "}\n",
    "lr_grid =GridSearchCV(estimator=lr,\n",
    "                      param_grid=param_lr,\n",
    "                      cv=5,\n",
    "                      scoring='accuracy')\n",
    "\n",
    "lr_grid.fit(X_train_full,y_train)\n",
    "\n",
    "print(\"time takes:\",round((time.time()-start)/60,1))\n",
    "print(lr_grid.best_params_)\n",
    "print(lr_grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "smart-feelings",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           F       0.93      0.92      0.92     12055\n",
      "           M       0.86      0.88      0.87      6950\n",
      "\n",
      "    accuracy                           0.90     19005\n",
      "   macro avg       0.89      0.90      0.90     19005\n",
      "weighted avg       0.90      0.90      0.90     19005\n",
      "\n",
      "testing score is 0.9033938437253355\n",
      "training score is 0.99293249601028\n"
     ]
    }
   ],
   "source": [
    "## testing score of the optimal model selected\n",
    "## highly overfitting\n",
    "lr_select = LogisticRegression(C=0.6,penalty='none', solver='saga',max_iter=80,warm_start=False)\n",
    "model_result(lr_select, X_train_full_over, y_train_over, X_val_full,y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-pavilion",
   "metadata": {},
   "source": [
    "The similar overfitting problem is highly likely to occur after tuning for rest models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forty-desire",
   "metadata": {},
   "source": [
    "## simple model ensemling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pursuant-scheme",
   "metadata": {},
   "source": [
    "using one model for tfidf/cv character n-gram matrix, another model for other crafted features, and combine the predicted probality see if there is an improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "sudden-aviation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           F       0.86      0.85      0.86     12055\n",
      "           M       0.75      0.77      0.76      6950\n",
      "\n",
      "    accuracy                           0.82     19005\n",
      "   macro avg       0.80      0.81      0.81     19005\n",
      "weighted avg       0.82      0.82      0.82     19005\n",
      "\n",
      "0.8193633254406735\n",
      "0.9296237832149434\n"
     ]
    }
   ],
   "source": [
    "# model 1: nb model for text\n",
    "X_train_tfv, X_val_tfv = vect_transfrom(vect_tfv, X_train, X_val)\n",
    "X_train_tfv_over,y_train_over = over.fit_resample(X_train_tfv,y_train)\n",
    "nb=MultinomialNB(alpha=0.1)\n",
    "nb.fit(X_train_tfv_over,y_train_over)\n",
    "y_pred = nb.predict(X_val_tfv)\n",
    "y_pred_train = nb.predict(X_train_tfv)\n",
    "print(classification_report(y_true=y_val,y_pred=y_pred))\n",
    "print(accuracy_score(y_val,y_pred))\n",
    "print(accuracy_score(y_train,y_pred_train))\n",
    "proba1=nb.predict_proba(X_val_tfv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "apparent-sunset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           F       0.89      0.89      0.89     12055\n",
      "           M       0.81      0.80      0.80      6950\n",
      "\n",
      "    accuracy                           0.86     19005\n",
      "   macro avg       0.85      0.84      0.85     19005\n",
      "weighted avg       0.86      0.86      0.86     19005\n",
      "\n",
      "0.856564062088924\n",
      "0.8660615627466456\n"
     ]
    }
   ],
   "source": [
    "# model 2: lr model for other extracted features\n",
    "X_train_manual = X_train.drop(columns=['clean_name'])\n",
    "X_val_manuel = X_val.drop(columns=['clean_name'])\n",
    "lr = LogisticRegression(random_state=1004)\n",
    "lr.fit(X_train_manual, y_train)\n",
    "y_pred = lr.predict(X_val_manuel)\n",
    "y_pred_train = lr.predict(X_train_manual)\n",
    "print(classification_report(y_true=y_val,y_pred=y_pred))\n",
    "print(accuracy_score(y_val,y_pred))\n",
    "print(accuracy_score(y_train,y_pred_train))\n",
    "proba2=lr.predict_proba(X_val_manuel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "known-religion",
   "metadata": {},
   "outputs": [],
   "source": [
    "proba =pd.DataFrame((proba1+proba2)/2)\n",
    "proba =proba.apply(np.argmax, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "latest-revelation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.92      0.91     12055\n",
      "           1       0.85      0.82      0.84      6950\n",
      "\n",
      "    accuracy                           0.88     19005\n",
      "   macro avg       0.88      0.87      0.87     19005\n",
      "weighted avg       0.88      0.88      0.88     19005\n",
      "\n",
      "0.8828203104446198\n"
     ]
    }
   ],
   "source": [
    "y_val_new=y_val.apply(lambda x: int(x=='M'))\n",
    "print(classification_report(y_true=y_val_new,y_pred=proba))\n",
    "print(accuracy_score(y_val_new,proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-dublin",
   "metadata": {},
   "source": [
    "The model accruacy doesnt seem to improve. Try change model 1 from nb to logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "stable-manual",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           F       0.92      0.89      0.90     12055\n",
      "           M       0.81      0.87      0.84      6950\n",
      "\n",
      "    accuracy                           0.88     19005\n",
      "   macro avg       0.87      0.88      0.87     19005\n",
      "weighted avg       0.88      0.88      0.88     19005\n",
      "\n",
      "0.8785056564062089\n",
      "0.9217442778216259\n"
     ]
    }
   ],
   "source": [
    "# model 1: lr model for text\n",
    "X_train_tfv, X_val_tfv = vect_transfrom(vect_tfv, X_train, X_val)\n",
    "X_train_tfv_over,y_train_over = over.fit_resample(X_train_tfv,y_train)\n",
    "lr1=LogisticRegression(random_state=1004)\n",
    "lr1.fit(X_train_tfv_over,y_train_over)\n",
    "y_pred = lr1.predict(X_val_tfv)\n",
    "y_pred_train = lr1.predict(X_train_tfv)\n",
    "print(classification_report(y_true=y_val,y_pred=y_pred))\n",
    "print(accuracy_score(y_val,y_pred))\n",
    "print(accuracy_score(y_train,y_pred_train))\n",
    "proba1=lr1.predict_proba(X_val_tfv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "deadly-camel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.90      0.91     12055\n",
      "           1       0.84      0.84      0.84      6950\n",
      "\n",
      "    accuracy                           0.88     19005\n",
      "   macro avg       0.87      0.87      0.87     19005\n",
      "weighted avg       0.88      0.88      0.88     19005\n",
      "\n",
      "0.8816101026045777\n"
     ]
    }
   ],
   "source": [
    "proba =pd.DataFrame((proba1+proba2)/2)\n",
    "proba =proba.apply(np.argmax, axis=1)\n",
    "y_val_new=y_val.apply(lambda x: int(x=='M'))\n",
    "print(classification_report(y_true=y_val_new,y_pred=proba))\n",
    "print(accuracy_score(y_val_new,proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medieval-rainbow",
   "metadata": {},
   "source": [
    "Similarly, the ensembling model accruacy is lower than previous lr model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-polls",
   "metadata": {},
   "source": [
    "## final model selected\n",
    "The selected model is the simple logistic regression model with default parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "controversial-trinidad",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['gender','name','name_len'])\n",
    "y = df['gender']\n",
    "vect_tfv=TfidfVectorizer(analyzer='char',ngram_range=(2,6))\n",
    "X_tfv = vect_tfv.fit_transform(X['clean_name'])\n",
    "X_full = combine_features(X, X_tfv)\n",
    "X_full_over, y_over = over.fit_resample(X_full, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "hollow-prerequisite",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=1004)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_select = LogisticRegression(random_state=1004)\n",
    "lr_select.fit(X_full_over,y_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "liked-complexity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.pkl']"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model\n",
    "import joblib\n",
    "joblib.dump(lr_select, 'model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objective-continuity",
   "metadata": {},
   "source": [
    "## for flask application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "severe-focus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_df(s):\n",
    "    s = ''.join([w for w in s if w.isalpha()])\n",
    "    df = pd.DataFrame([s], columns=['clean_name'])\n",
    "    return df\n",
    "    \n",
    "def process_input(s):\n",
    "    df= to_df(s)\n",
    "    df['last_letter']=df['clean_name'].apply(lambda x: x[-1])\n",
    "    df['last_2_letter']=df['clean_name'].apply(lambda x:x[-2:])\n",
    "    df['last_3_letter']=df['clean_name'].apply(lambda x:x[-3:])\n",
    "    df['ild']=df['clean_name'].apply(lambda x: bool(re.search('lid',x)))\n",
    "    df['lin']=df['clean_name'].apply(lambda x: bool(re.search('lin',x)))\n",
    "    features_col=['last_letter','last_2_letter','last_3_letter']\n",
    "    new_cols = onehot.transform(df[features_col]).toarray()\n",
    "    names_col=onehot.get_feature_names(features_col)\n",
    "    df_new= pd.DataFrame(new_cols,dtype=bool, columns=names_col)\n",
    "    df=pd.concat([df,df_new],axis=1).drop(columns=features_col)\n",
    "    X_tfv = vect_tfv.transform(df['clean_name'])\n",
    "    X_full = combine_features(df, X_tfv)\n",
    "    return X_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "every-fusion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 203740) (95025, 203740)\n"
     ]
    }
   ],
   "source": [
    "print(process_input('Emily').shape,X_full.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foster-strategy",
   "metadata": {},
   "source": [
    "## other thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insured-remark",
   "metadata": {},
   "source": [
    "word2vec/BERT + neural network is not implemented since:  \n",
    "(1) its very time consuming (2) not sure the perfermance of charater based BERT "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
